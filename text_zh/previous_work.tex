\section{既往工作与当前趋势}\label{sec:previouswork}

自以太坊 \emph{YP} 最初发表以来，区块链开发领域迅猛发展。除可扩展性外，研究与工程也围绕底层共识算法、智能合约语言与虚拟机以及整体状态环境展开。虽同样有趣，但这些主题多数不在本文范围之内，因为它们通常并不直接影响底层的可扩展性。

\subsection{Polkadot}

为交付其服务，\Jam 复用了与 Polkadot 相同的大量博弈论与密码学机制（称作 \textsc{Elves}，见 \cite{cryptoeprint:2024/961}）。然而，\Jam 在实际提供的服务上存在重大差异，其抽象更贴近“由验证人节点在经济激励下生成的实际计算模型”。

最初的 Polkadot 提案——“可扩展的异构多链”——的重要观点，是通过在多台主机之间分割与分发工作负载来实现高性能。在这样做的同时，它明确承认可组合性会降低。Polkadot 的组成部分（平行链）在实践中高度隔离。尽管存在消息传递系统（\textsc{xcmp}），但它是异步且粒度较粗的，并在很大程度上受限于其所依赖的高层、演化缓慢的交互语言 \textsc{xcm}。

因此，相较于提供“单一且通用对象环境、并允许灵活创新集成”的以太坊式智能合约系统，Polkadot 在其组成链之间所提供的可组合性更低。以当前形态看，Polkadot 更像是由多个相互独立的生态组成，彼此协作的机会有限，其人体工学体验与跨链桥相似，尽管其安全性质完全不同。一个名为 \textsc{spree} 的技术提案打算利用 Polkadot 独特的共享安全来提升可组合性，不过各区块链仍然保持隔离。

实现并启动一条区块链很难、耗时且成本高昂。基于其最初设计，Polkadot 将可使用其服务的客户端限定为：既能够完成上述工作，又能筹集足够押金以在拍卖中赢得长期插槽的一方（目前大约仅 50 个）。虽不属于严格意义的“许可制”，但与类似以太坊的智能合约系统相比，其可达性确实显著更低。

让尽可能多的创新者参与并彼此互动（也与彼此的用户群互动），似乎是 Web3 应用平台成功的重要组成部分。因此，可达性至关重要。

\subsection{Ethereum}

以太坊协议由 \cite{wood2014ethereum} 在本文的“精神前作”——\emph{黄皮书}——中正式定义，其思想很大程度上源自 \cite{buterin2013ethereum} 的初始白皮书。自 \emph{YP} 发表十年以来，“事实上的”以太坊协议与其公共网络经历了多次演进，主要围绕“通过交易格式、指令集及其脚本核心（以太坊虚拟机 \textsc{evm}）的‘预编译’（小众且复杂的额外指令）”来引入灵活性。

接近一百万的加密经济参与者参与以太坊的验证。\footnote{现实因素限制了去中心化的\emph{实际}水平。验证者软件明确提供功能，使单个实例可配置多组密钥，这系统性地降低了“实际去中心化程度”，无论在“运营者”层面还是“硬件”层面。根据 \cite{hildobby2024eth2} 汇总的以太坊 2 数据，可以看到一个主要节点运营商 Lido 长期占据近三分之一的参与者份额。} 区块扩展通过“随机的领导者轮换”完成，且领导者的物理地址在其出块之前是公开的。\footnote{以太坊开发者希望改进为更安全的方式，但尚无固定时间表。} 以太坊使用 \cite{buterin2019casper} 引入的 Casper-\textsc{ffg} 实现最终性；在庞大验证者基数下，大约每 13 分钟最终确定一次链式扩展。

以太坊的直接计算性能与 2015 年上线时大体相当，但有一处显著例外：现在每个区块允许额外托管 1\textsc{mb} 的\emph{承诺数据}（所有节点在有限时间内存储）。该数据无法被主状态转换函数直接使用，但可通过特殊函数证明“该数据（或其子集）可用”。据 \cite{ethereum2024danksharding}，当前的设计方向是在未来几年内将其改进为 \emph{Dank-sharding}：通过在验证者之间分割存储责任来实现。

据 \cite{ethereum2024sigital}，以太坊的扩容策略将“数据可用性”与一个由\emph{Rollup} 组成的“私有市场”结合起来——即各种设计的带外计算设施，且明确倾向于基于 \textsc{zk-snark} 的 Rollup。不同厂商的 Rollup 在设计、执行与运营上均有其各自影响。

可以合理地假设：“通过多厂商 Rollup 的市场化扩容”将使设计优良的方案脱颖而出。然而，该策略也可能面临问题。 \cite{sharma2024ethereums} 对各类 Rollup 的去中心化程度开展研究后发现：整体存在“中心化”趋势，但也指出业界正努力加以缓解。它们究竟能被做得多去中心化，仍有待观察。

在数据报延迟与语义范围等通信性质、安全性质（如：回滚、篡改、停摆与审查的代价）、经济性质（如：接受与处理某条入站消息或交易的成本）等方面，不同厂商的 Rollup 拼接而成的“大拼图”之间，可能存在显著差异。即便以太坊网络最终为带外计算提供了所需的大多数（甚至全部）底层机制，我们仍无法确认“各种性质会否最终在宏观上整合”。我们尚未找到关于这种碎片化路径在负面影响上的充分讨论。\footnote{对此的一些初步思考促成了 \cite{sadana2024bringing} 的提案，建议利用 Polkadot 技术来在 Rollup 生态之间提供一定的兼容性！}

\subsubsection{\textsc{Snark} Roll-up}

尽管以太坊协议的基础并未对 Rollup 的性质作强假设，但其带外计算的策略确实以 \textsc{snark} 为中心，并正向适配这一方向演化。\textsc{Snark} 属于一种“非同寻常的密码学”，它允许构造证明，向旁观者表明某预定义计算的结果是正确的。验证复杂度通常次线性于被证明计算规模，且不会泄露该计算的内部细节，亦不泄露其所依赖的见证数据。

\textsc{Zk-snark} 伴随约束：证明大小、验证复杂度与生成复杂度之间存在权衡。非平凡计算，尤其是“富含二进制操作、而这正是智能合约吸引力所在的通用计算”，很难适配进 \textsc{snark} 的模型。

以一个实际例子而言，\textsc{risc}-zero（参见 \cite{bogli2024assessing}）是领先项目之一，它提供了在 \textsc{risc-v} 虚拟机上运行的计算生成 \textsc{snark} 的平台。\textsc{risc-v} 是开源且精炼的 \textsc{risc} 架构，工具链完善。 \cite{koute2024risc0} 的近期基准报告显示：相较 \textsc{risc}-zero 自身的基准，即便使用 32 倍核心、20,000 倍 \textsc{ram}，再加一块最先进的 \textsc{gpu}，\emph{仅证明生成}就需要比“简单地重编译并执行”\emph{多出} 61,000 倍以上的时间。按硬件租赁商 \url{https://cloud-gpus.com/} 的报价，以 \textsc{risc}-zero 进行证明的成本倍率约为使用 Polka\textsc{vm} 重编译器执行的 66,000,000 倍\footnote{很可能实际更高，因为他们使用的是消费级“闲置”低配硬件，且我们的重编译器尚未优化。}。

许多密码学原语在该框架中会变得过于昂贵而不切实际，必须以专用算法与结构替代，而这些替代往往在其他方面次优。由于预期采用 \textsc{snark}（如 \cite{cryptoeprint:2019/953} 提出的 \textsc{plonk}），以太坊的 Dank-sharding 数据可用性系统偏向使用“大素数域上的多项式承诺中心化的纠删码”，以便 \textsc{snark} 能以可接受的性能访问子数据块。与本文采用“二元域加 Merklization”的替代方案相比，这会让验证节点在 \textsc{cpu} 使用上承受数量级更高的负载。

除了基本成本之外，\textsc{snark} 并不能带来“摆脱去中心化与冗余需求”的灵丹妙药，反而引入更多成本倍数。尽管其“可验证性”在某种程度上减轻了对“抵押型去中心化”的需求，但为了防止某一家形成垄断（或几家形成卡特尔），仍需激励多个独立方执行大致相同的工作。虽然“证明错误状态转换”在理论上应是不可能的，但服务完整性仍可能以其他方式受损；哪怕只是几分钟的证明生成停摆，也可能对实时金融应用造成巨大的经济影响。

现实中由中心化陷阱引发的“垄断”并非没有先例。前述的某类基于 \textsc{snark} 的交易撮合框架即是一例：其名义上服务于去中心化交易所，实际上却由 Starkware 独占“生成并提交证明以执行交易”的权力——导致单点故障：若 Starkware 的服务受损，系统的活性将受影响。

尚无证据表明：以 \textsc{snark} 为核心、旨在“从计算中消弭信任假设”的策略，能在成本上与“多方加密经济平台”竞争。截至目前，所有已提出的 \textsc{snark} 方案都在很大程度上依赖加密经济系统来为其“定框”并规避其问题。数据可用性与排序是两个公认必须靠加密经济方案解决的领域。

需要指出的是，\textsc{snark} 技术正在进步，其背后的密码学家与工程师预计未来数年会有改良。 \cite{thaler2023technical} 的文章提出了可信的推测：随着近期密码技术的进展，证明生成的减速因子可能低至“相对原生执行仅 50,000 倍”，且其中很大一部分可以并行化。这比现状好得多，但距离与 \textsc{Elves} 等成熟的加密经济技术在成本上竞争，仍有几个数量级的差距。

\subsection{碎片化的元网络}

其他项目在“通用计算可扩展性”上的方向，大致集中在两条路径：\emph{碎片化}或\emph{中心化}。我们认为两者都难言有说服力。

碎片化的路径由 Cosmos（\cite{kwon2019cosmos}）与 Avalanche（\cite{tanana2019avalanche}）等项目引领。其做法是：系统被碎片化为“采用同构共识机制的网络组”，但每个网络由各自独立、动机各异的验证者组成。这不同于 Polkadot 的“单一验证者集”，也不同于以太坊“由同一验证者集在一致激励框架下部分保障、但 Rollup 异构”的策略。同构带来的“消息机制一致性”确实能在接口层呈现出更统一的外观。

然而，这种一致性是表面的。所谓“无信任”，仍需假设验证者“正确行事”；而验证者是在由经济激励与惩罚最终塑造的“加密经济安全框架”下运作。若想“在不对验证者集作特殊协调的前提下，以相同安全级别做到两倍工作量”，此类系统的处方实际上就是：再造一个网络，并给予与现有网络同等的激励与惩戒。

由此引出若干问题。首先，与 Polkadot 的“孤立平行链”以及以太坊的“孤立 Rollup”相似，该路径同样存在“持续分片的状态导致无法同步可组合”之弊端。

更麻烦的是：Cosmos 提出的“以碎片化扩容”的方案，并未给出“同构安全（进而无信任）”保证。不同网络之间的验证者集被假定为“独立选择与激励”，其拜占庭行为在一个网络上的出现，与在另一个网络上应受何种惩处之间，不存在因果或概率上的关联。实质上，这意味着：若验证者在某一网络上串谋篡改或回滚状态，其影响可能波及该生态的其他网络。

大家普遍承认这是个问题，并提出两类解决思路。其一是“从同一验证者集汲取、统一跨网络的攻击成本（即安全级别）”。如 \cite{cosmos2024interchain} 所提的 \emph{replicated security}：要求每个验证者在所有网络上同时验证，且面对相同激励与惩罚。这在安全供给成本上极不经济：每个网络都必须独立地提供与“其希望对接的最安全网络”相当的激励与惩罚，以保证“对验证者的经济命题不变、对所有网络的安全命题等价”。目前，replicated security 并非随取随用的“无许可服务”。我们可以推测：这背后“代价高昂的经济学”是原因之一。

效率更高的路径由 OmniLedger 团队提出（\cite{cryptoeprint:2017/406}）：让验证者“非冗余”，在不同网络间分片，并定期、以安全随机的方式重分片。相较“全员验证单一网络”的方案，其攻击成本有所下降，因为总体系中恶意比例虽不足，但在某次随机分片中，个别网络仍有一定概率被分配到“临界比例”的恶意验证者。尽管如此，该方案在“弱一致性”前提下是可行的扩容之道。

或者，如 \cite{cryptoeprint:2024/961} 的 \textsc{Elves} 所示：使用“非冗余分片”，辅以“提议—审计博弈”来识别并惩处无效计算，并将“某网络的最终性”依赖于“所有与其因果纠缠的网络”。三者而言，这是“最安全且最经济高效”的方案：因为存在一种机制，可在“其影响在整个网络族被最终确定之前”以高置信度识别并修正无效转换。然而，它需要更复杂的逻辑，同时“因果纠缠”也为“可新增网络的规模”设置了上限。

\subsection{高性能的完全同步网络}

区块链近年的另一趋势，是通过“战术性优化”来提升吞吐量：比如限制验证者集规模或多样性、聚焦软件优化、要求验证者间更高的一致性、对验证者硬件提出更苛刻要求，或限制数据可用性。

Solana 区块链基于 \cite{yakovenko2018solana} 引入的技术，其理论峰值号称每秒 70 万笔交易；然而据 \cite{ng2024is}，实际网络仅处理其中的一小部分。其底层吞吐量仍显著高于多数链，得益于围绕“最大化同步性能”的多项工程优化。结果是一个高度一致的智能合约环境，其 \textsc{api} 与 \emph{YP} 以太坊（尽管底层 \textsc{vm} 不同）相仿，且上链几乎即时，最终性亦被视为“随纳入即最终”。

此路数带来两点问题：其一，将协议“定义为一套重度优化代码库的产出”会带来结构性中心化，削弱韧性。 \cite{jha2024solana} 记载：“自 2022 年 1 月起，Solana 发生 11 次重大故障，共计 15 天出现全部或部分停摆”。这在主流区块链中属于离群，因为绝大多数主流链从不宕机。停摆原因多样，但通常由各子系统中的 Bug 触发。

相对地，以太坊（至少在近年之前）提供了鲜明对比：其规范审阅充分、加密经济基础研究清晰且有多套“洁净室实现”。因此，当最广泛部署的实现出现缺陷并遭恶意利用时，网络仍基本不受影响地持续运行（见 \cite{hertig2016so}）。

其二，在“协议不提供将工作负载分布到单机之外的能力”的前提下，其极限可扩展性令人担忧。

在大规模使用场景中，“历史交易数据与状态”都会增长到不切实际。Solana 直观地示范了问题何等棘手。与经典区块链不同，Solana 协议未对“历史数据归档与后续可复核”提供方案——而这对于第三方从第一性原理证明“当前状态正确”至关重要。文献中关于 Solana 如何处理此事的信息很少，据 \cite{solana2023solana}，节点只是将数据放入由 Google 托管的中心化数据库。\footnote{早期节点版本曾使用去中心化数据存储网络 Arweave，但被证明无法满足 Solana 的吞吐需求。}

Solana 鼓励验证者安装大容量 \textsc{ram} 以将庞大状态留在内存中（据 \cite{solana2024solana}，当前建议为 512 \textsc{gb}）。在没有“分而治之”的前提下，Solana 表明：“验证者可合理提供的硬件水平”决定了“完全同步、完全一致执行模型”的性能上限。硬件门槛构成验证者准入壁垒；若继续抬高，势必牺牲去中心化，最终也损害透明度。
